{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6ce3648",
   "metadata": {},
   "source": [
    "# [Take-home Assessment] Food Crisis Early Warning \n",
    "\n",
    "Welcome to the assessment. You will showcase your modeling and research skills by investigating news articles (in English and Arabic) as well as a set of food insecurity risk factors. \n",
    "\n",
    "We suggest planning to spend **~6–8 hours** on this assessment. **Please submit your response by Monday, September 15th, 9:00 AM EST via email to dime_ai@worldbank.org**. Please document your code with comments and explanations of design choices. There is one question on reflecting upon your approach at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cf2966",
   "metadata": {},
   "source": [
    "**Name:**  Jonas Nothnagel\n",
    "\n",
    "**Email:** jonas.nothnagel@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb109a3",
   "metadata": {},
   "source": [
    "# Part 1: Technical Assessment\n",
    "\n",
    "\n",
    "## Task:\n",
    "\n",
    "We invite you to approach the challenge of understanding (and potentially predicting) food insecurity using the provided (limited) data. Your response should demonstrate how you tackle open-ended problems in data-scarce environments.\n",
    "\n",
    "Some example questions to consider:\n",
    "- What is the added value of geospatial data?\n",
    "- How can we address the lack of ground-truth information on food insecurity levels?\n",
    "- What are the benefits and challenges of working with multilingual data?\n",
    "- ...\n",
    "\n",
    "These are just guiding examples — you are free to explore any relevant angles to this topic/data.\n",
    "\n",
    "**Note:** There is no single \"right\" approach. Instead, we want to understand how you approach and structure open-ended problems in data-scarce environments. Given the large number of applicants, we will preselect the most impressive and complete submissions. Please take effort in structuring your response, as selection will depend on its depth and originality.\n",
    "\n",
    "\n",
    "## Provided Data:\n",
    "\n",
    "1. **Risk Factors:** A file containing 167 risk factors (unigrams, bigrams, and trigrams) in the `english_keywords` column and an empty `keywords_arabic` column. A separate file with the mapping of English risk factors to pre-defined thematic cluster assignments.\n",
    "\n",
    "\n",
    "2. **News Articles:** Two files containing one month of news articles from the Mashriq region:\n",
    "   - `news-articles-eng.csv`\n",
    "   - `news-articles-ara.csv`\n",
    "   - **Note:** You may work on a sample subset during development.\n",
    "   \n",
    "   \n",
    "3. **Geographic Taxonomy:** A file containing the names of the countries, provinces, and districts for the subset of Mashriq countries that is covered by the news articles. The files are a dictionary mapping from a key to the geographic name.\n",
    "   - `id_arabic_location_name.pkl`\n",
    "   - `id_english_location_name.pkl`\n",
    "   - **Note:** Each unique country/province/district is assigned a key (e.g. `iq`,`iq_bg` and `iq_bg_1` for country Iraq, province Baghdad, and district 1 in Baghdad respectively).\n",
    "   - The key of country names is a two character abbreviation as follows.\n",
    "       - 'iq': 'Iraq'\n",
    "       - 'jo': 'Jordan'\n",
    "       - 'lb': 'Lebanon'\n",
    "       - 'ps': 'Palestine'\n",
    "       - 'sy': 'Syria'\n",
    "       \n",
    "   - The key of provinces is a two-character abbreviation of the country followed by two-character abbreviation of the province **`{country_abbreviation}_{province_abbreviation}`**, and the key of districts is **`{country_abbreviation}_{province_abbreviation}_{unique_number}`**.\n",
    "       \n",
    "\n",
    "\n",
    "## Submission Guidelines:\n",
    "\n",
    "- **Code:** Follow best coding practices and ensure clear documentation. All notebook cells should be executed with outputs saved, and the notebook should run correctly on its own. Name your file **`solution_{FIRSTNAME}_{LASTNAME}.ipynb`**. If your solution relies on additional open-access data, either include it in your submission (e.g., as part of a ZIP file) or provide clear data-loading code/instructions as part of the nottebook. \n",
    "- **Report:** Submit a separate markdown file communicating your approach to this research problem. We expect you to detail the models, methods, or (additional) data you are using.\n",
    "\n",
    "Good luck!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09329a02",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f9934a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task1",
   "metadata": {},
   "source": [
    "## Your Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "475a9a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a12c3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data from previous steps that we stored under new_data folder\n",
    "news_articles_english = pd.read_excel('new_data/english_news_clean.xlsx')\n",
    "news_articles_arabic = pd.read_excel('new_data/arabic_news_clean.xlsx')\n",
    "cluster_df = pd.read_excel('new_data/bilingual_clusters.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ce6f2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create non-random subset with seed to ensure reproducibility for easier sampling and development.\n",
    "# I opt to use a non-random sample to keep the time series structure of the data intact.\n",
    "# Let's pick a block of n rows from a random start index though.\n",
    "n_rows_en = len(news_articles_english)\n",
    "n_rows_ara = len(news_articles_arabic)\n",
    "block_size = 10000\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# pick a random start index so that the slice fits inside the dataframe\n",
    "start_en = np.random.randint(0, n_rows_en - block_size + 1)\n",
    "start_ara = np.random.randint(0, n_rows_ara - block_size + 1)\n",
    "\n",
    "# slice the dataframe\n",
    "subset_new_en = news_articles_english.iloc[start_en:start_en + block_size].reset_index(drop=True)\n",
    "subset_new_ara = news_articles_arabic.iloc[start_ara:start_ara + block_size].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23728a9a",
   "metadata": {},
   "source": [
    "### Step 1 of Analysis - Match Clusters to articles:\n",
    "- Several ways to do this. It depends how much Precision/Recall we want, and wether we opt for open-source tools (yes bcs reproducibility) or more conservative approaches such us simple keyword matching. I will opt for using sentence transformers to account for semantic similiarity but will probably set the treshhold on a more higher level for higher precision. For this we need to embedd both titles/body as well as keywords/clusters. \n",
    "- We could think of some sort of hierarchical semantic similarity matching prioritising the title.\n",
    "- BIG Question: Do we allow articles to enter multiple clusters or only one?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f304389f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualitative",
   "metadata": {},
   "source": [
    "# Part 2: Reflection\n",
    "\n",
    "Please outline (1) some of the limitations of your approach and (2) how you would tackle these if you had more time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49236dab",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
